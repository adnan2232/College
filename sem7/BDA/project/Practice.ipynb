{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3effc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90960587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 12:20:57 WARN Utils: Your hostname, adnan-System-Product-Name resolves to a loopback address: 127.0.1.1; using 192.168.1.109 instead (on interface enp2s0)\n",
      "22/10/04 12:20:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 12:20:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.109:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice_session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fceb659d9c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"practice_session\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "375699a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@pandas_udf(\"int\")\n",
    "def calAvg(series:pd.Series)->float:\n",
    "    return series.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5bc9661f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Name</th><th>age</th><th>experience</th><th>salary</th></tr>\n",
       "<tr><td>Krish</td><td>31</td><td>10</td><td>30000</td></tr>\n",
       "<tr><td>Sudhanshu</td><td>30</td><td>8</td><td>25000</td></tr>\n",
       "<tr><td>Sunny</td><td>29</td><td>4</td><td>20000</td></tr>\n",
       "<tr><td>Paul</td><td>24</td><td>3</td><td>20000</td></tr>\n",
       "<tr><td>Harsha</td><td>21</td><td>1</td><td>15000</td></tr>\n",
       "<tr><td>Shubham</td><td>23</td><td>2</td><td>18000</td></tr>\n",
       "<tr><td>Mahesh</td><td>28</td><td>0</td><td>40000</td></tr>\n",
       "<tr><td>unknown</td><td>34</td><td>10</td><td>38000</td></tr>\n",
       "<tr><td>unknown</td><td>36</td><td>0</td><td>28</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---+----------+------+\n",
       "|     Name|age|experience|salary|\n",
       "+---------+---+----------+------+\n",
       "|    Krish| 31|        10| 30000|\n",
       "|Sudhanshu| 30|         8| 25000|\n",
       "|    Sunny| 29|         4| 20000|\n",
       "|     Paul| 24|         3| 20000|\n",
       "|   Harsha| 21|         1| 15000|\n",
       "|  Shubham| 23|         2| 18000|\n",
       "|   Mahesh| 28|         0| 40000|\n",
       "|  unknown| 34|        10| 38000|\n",
       "|  unknown| 36|         0|    28|\n",
       "+---------+---+----------+------+"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"practice_data.csv\",header=True,inferSchema=True)\n",
    "df.na.fill({\n",
    "    \"age\":df.select(calAvg(df.age)).collect()[0][0],\n",
    "    \"name\":\"unknown\",\n",
    "    \"experience\":0,\n",
    "    \"salary\":df.select(calAvg(df.age)).collect()[0][0],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "710f4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([\n",
    "    Row(Name=\"Krish\",Departments=\"Data Science\",Salary=10000),\n",
    "    Row(Name=\"Krish\",Departments=\"IOT\",Salary=5000),\n",
    "    Row(Name=\"Krish\",Departments=\"Big Data\",Salary=4000),\n",
    "    Row(Name=\"Sudhanshu\",Departments=\"IOT\",Salary=20000),\n",
    "    Row(Name=\"Sudhanshu\",Departments=\"Data Science\",Salary=10000),\n",
    "    Row(Name=\"Sudhanshu\",Departments=\"Big Data\",Salary=5000),\n",
    "    Row(Name=\"Sunny\",Departments=\"Data Science\",Salary=10000),\n",
    "    Row(Name=\"Sunny\",Departments=\"Big Data\",Salary=2000),\n",
    "    Row(Name=\"Mahesh\",Departments=\"Data Science\",Salary=4000),\n",
    "    Row(Name=\"Mahesh\",Departments=\"Big Data\",Salary=3000)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "64499117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Mahesh', sum(Salary)=7000)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df2.groupBy(\"Name\").sum().collect(),key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f3ed37b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Name</th><th>sum(Salary)</th></tr>\n",
       "<tr><td>Krish</td><td>19000</td></tr>\n",
       "<tr><td>Sudhanshu</td><td>35000</td></tr>\n",
       "<tr><td>Sunny</td><td>12000</td></tr>\n",
       "<tr><td>Mahesh</td><td>7000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-----------+\n",
       "|     Name|sum(Salary)|\n",
       "+---------+-----------+\n",
       "|    Krish|      19000|\n",
       "|Sudhanshu|      35000|\n",
       "|    Sunny|      12000|\n",
       "|   Mahesh|       7000|\n",
       "+---------+-----------+"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupBy(\"Name\").sum(\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "96b75ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Name</th><th>Departments</th><th>Salary</th></tr>\n",
       "<tr><td>Krish</td><td>Data Science</td><td>3666</td></tr>\n",
       "<tr><td>Krish</td><td>IOT</td><td>-1333</td></tr>\n",
       "<tr><td>Krish</td><td>Big Data</td><td>-2333</td></tr>\n",
       "<tr><td>Mahesh</td><td>Data Science</td><td>500</td></tr>\n",
       "<tr><td>Mahesh</td><td>Big Data</td><td>-500</td></tr>\n",
       "<tr><td>Sudhanshu</td><td>IOT</td><td>8333</td></tr>\n",
       "<tr><td>Sudhanshu</td><td>Data Science</td><td>-1666</td></tr>\n",
       "<tr><td>Sudhanshu</td><td>Big Data</td><td>-6666</td></tr>\n",
       "<tr><td>Sunny</td><td>Data Science</td><td>4000</td></tr>\n",
       "<tr><td>Sunny</td><td>Big Data</td><td>-4000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+------------+------+\n",
       "|     Name| Departments|Salary|\n",
       "+---------+------------+------+\n",
       "|    Krish|Data Science|  3666|\n",
       "|    Krish|         IOT| -1333|\n",
       "|    Krish|    Big Data| -2333|\n",
       "|   Mahesh|Data Science|   500|\n",
       "|   Mahesh|    Big Data|  -500|\n",
       "|Sudhanshu|         IOT|  8333|\n",
       "|Sudhanshu|Data Science| -1666|\n",
       "|Sudhanshu|    Big Data| -6666|\n",
       "|    Sunny|Data Science|  4000|\n",
       "|    Sunny|    Big Data| -4000|\n",
       "+---------+------------+------+"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calAvg(frame):\n",
    "    return frame.assign(Salary=frame.Salary-frame.Salary.mean())\n",
    "    \n",
    "df2.groupBy(\"Name\").applyInPandas(calAvg,schema=df2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9069191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill({\n",
    "    \"age\":df.summary(\"mean\").collect()[0].asDict()[\"age\"],\n",
    "    \"name\":\"Unknown\",\n",
    "    \"experience\":0,\n",
    "    \"salary\":df.summary(\"min\").collect()[0].asDict()[\"salary\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "45e6d640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Name</th><th>age</th><th>experience</th><th>salary</th></tr>\n",
       "<tr><td>Krish</td><td>31</td><td>10</td><td>30000</td></tr>\n",
       "<tr><td>Sudhanshu</td><td>30</td><td>8</td><td>25000</td></tr>\n",
       "<tr><td>Sunny</td><td>29</td><td>4</td><td>20000</td></tr>\n",
       "<tr><td>Paul</td><td>24</td><td>3</td><td>20000</td></tr>\n",
       "<tr><td>Harsha</td><td>21</td><td>1</td><td>15000</td></tr>\n",
       "<tr><td>Shubham</td><td>23</td><td>2</td><td>18000</td></tr>\n",
       "<tr><td>Mahesh</td><td>28</td><td>0</td><td>40000</td></tr>\n",
       "<tr><td>Unknown</td><td>34</td><td>10</td><td>38000</td></tr>\n",
       "<tr><td>Unknown</td><td>36</td><td>0</td><td>15000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---+----------+------+\n",
       "|     Name|age|experience|salary|\n",
       "+---------+---+----------+------+\n",
       "|    Krish| 31|        10| 30000|\n",
       "|Sudhanshu| 30|         8| 25000|\n",
       "|    Sunny| 29|         4| 20000|\n",
       "|     Paul| 24|         3| 20000|\n",
       "|   Harsha| 21|         1| 15000|\n",
       "|  Shubham| 23|         2| 18000|\n",
       "|   Mahesh| 28|         0| 40000|\n",
       "|  Unknown| 34|        10| 38000|\n",
       "|  Unknown| 36|         0| 15000|\n",
       "+---------+---+----------+------+"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f402623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"age\",\"experience\"],outputCol=\"Independent_Feature\")\n",
    "indFeat = assembler.transform(df)\n",
    "final_data = indFeat.select([\"Independent_Feature\",\"salary\"])\n",
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "86114739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 19:37:11 WARN Instrumentation: [5f7c950e] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_7ab103419288, numFeatures=2"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "regression= LinearRegression(featuresCol=\"Independent_Feature\",labelCol=\"salary\")\n",
    "regression.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "1d68a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 19:38:42 WARN Instrumentation: [f6b6c9c8] regParam is zero, which might cause numerical instability and overfitting.\n",
      "+-------------------+------+------------------+\n",
      "|Independent_Feature|salary|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|         [21.0,1.0]| 15000|25420.650730411795|\n",
      "|         [23.0,2.0]| 18000|25830.677290836742|\n",
      "+-------------------+------+------------------+\n",
      "\n",
      "22/10/04 20:08:47 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-2e0220e7-4da3-40e1-a4de-ef5bc30c659c. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-2e0220e7-4da3-40e1-a4de-ef5bc30c659c\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:365)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2016)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2140)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52200)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/slowgamer/anaconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "regression.fit(train).evaluate(test).predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51062592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
